from flask import Blueprint, render_template, request, jsonify, session
import pandas as pd
import numpy as np
import pickle
import re
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModel
from langchain.docstore.document import Document
import hashlib
import torch
from sklearn.metrics.pairwise import cosine_similarity

search_bp = Blueprint('search', __name__)

# ë°ì´í„° ë¡œë“œ
cos_data_file = r'c:\projects\cosmetic\cos\data\Cos_data set_v2.9.csv'
cos_data_df = pd.read_csv(cos_data_file, encoding='utf-8-sig')

# âœ… ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_ingredients(ingredients):
    if pd.isna(ingredients):
        return ''
    ingredients = re.sub(r'[a-zA-Z]', '', ingredients)  # ì•ŒíŒŒë²³ ì œê±°
    ingredients = re.sub(r'[_\-]', '', ingredients)  # ì–¸ë”ë°” ë° ëŒ€ì‰¬ ì‚­ì œ
    ingredients = re.sub(r'\([^)]*\)', '', ingredients)  # ì†Œê´„í˜¸ ë‚´ìš© ì‚­ì œ
    ingredients = re.sub(r'\[[^\]]*\]', '', ingredients)  # ëŒ€ê´„í˜¸ ë‚´ìš© ì‚­ì œ
    ingredients = re.sub(r'[?]', '', ingredients)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return ingredients

# ì „ì²˜ë¦¬ ë° ê²°ì¸¡ì¹˜ ì œê±°
cos_data_df = cos_data_df.dropna(subset=['ingredients']).reset_index(drop=True)
cos_data_df['ingredients'] = cos_data_df['ingredients'].apply(preprocess_ingredients)

# âœ… ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

# âœ… ë²¡í„° í¬ê¸° í™•ì¸
sample_vector = embedding_model.embed_query("í…ŒìŠ¤íŠ¸ ë¬¸ì¥")
vector_dim = len(sample_vector)

# âœ… ì„±ë¶„ê³¼ ì„¤ëª…ì„ ê²°í•©í•˜ì—¬ ë²¡í„° ìƒì„± (ê°€ì¤‘ì¹˜ ì ìš©)
def get_combined_embedding(text1, text2, weight1=0.9, weight2=0.1):
    vec1 = embedding_model.embed_query(text1) if text1 else np.zeros(vector_dim)
    vec2 = embedding_model.embed_query(text2) if text2 else np.zeros(vector_dim)
    return weight1 * np.array(vec1) + weight2 * np.array(vec2)

# âœ… í”¼í´ íŒŒì¼ ë¡œë”© ë° FAISS ì¸ë±ìŠ¤ ìƒì„±
try:
    # í”¼í´ íŒŒì¼ ë¡œë”©
    with open('vectorstore.pkl', 'rb') as f:
        vectorstore = pickle.load(f)
except FileNotFoundError:
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    documents = [
        Document(
            page_content=row['ingredients'],
            metadata={
                'brand_name': row['brand_name'],
                'product_name': row['product_name'],
                'description': row['description'],
                'price': row['price'],
                'image_url': row['image_url'],
                'detail_url': row['detail_url']
            }
        )
        for _, row in cos_data_df.iterrows()
    ]
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    vectorstore = FAISS.from_documents(documents, embedding=embedding_model)

    # í”¼í´ íŒŒì¼ë¡œ ì €ì¥
    with open('vectorstore.pkl', 'wb') as f:
        pickle.dump(vectorstore, f)

def calculate_similarity(doc, ingredient_list):
    if not ingredient_list:
        return 0.0

    ingredient_vectors = np.array([embedding_model.embed_query(ing) for ing in ingredient_list])
    avg_ingredient_vector = np.mean(ingredient_vectors, axis=0)

    doc_embedding = np.array(embedding_model.embed_query(doc.page_content))
    return cosine_similarity([doc_embedding], [avg_ingredient_vector])[0][0]


# âœ… ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ (í•„í„°ë§ ì¶”ê°€)
def search_similar_cosmetics(ingredient_list, top_k=5):
    if not ingredient_list:
        return []

    ingredient_vectors = [embedding_model.embed_query(preprocess_ingredients(ing)) for ing in ingredient_list]
    query_vector = np.mean(ingredient_vectors, axis=0)

    results = vectorstore.similarity_search_with_score_by_vector(query_vector, k=top_k)
    hashed_results = {}

    for doc, score in results:
        doc_hash = hashlib.sha256((doc.page_content + doc.metadata['brand_name'] + doc.metadata['product_name']).encode('utf-8')).hexdigest()
        hashed_results[doc_hash] = (doc, score)

    if len(results) < top_k:
        remaining = top_k - len(results)
        candidate_docs = []
        
        for _, row in cos_data_df.iterrows():
            doc_vector = embedding_model.embed_query(row['ingredients'])
            similarity = cosine_similarity([doc_vector], [query_vector])[0][0]
            if similarity > 0.6:  # ìœ ì‚¬ë„ê°€ ì¼ì • ê¸°ì¤€ ì´ìƒì¼ ë•Œë§Œ ì¶”ê°€
                candidate_docs.append((row, similarity))

        candidate_docs = sorted(candidate_docs, key=lambda x: x[1], reverse=True)[:remaining]

        for row, similarity in candidate_docs:
            doc = Document(
                page_content=row['ingredients'],
                metadata={
                    'brand_name': row['brand_name'],
                    'product_name': row['product_name'],
                    'description': row['description'],
                    'price': row['price'],
                    'image_url': row['image_url'],
                    'detail_url': row['detail_url']
                }
            )
            doc_hash = hashlib.sha256((doc.page_content + doc.metadata['brand_name'] + doc.metadata['product_name']).encode('utf-8')).hexdigest()
            if doc_hash not in hashed_results:
                hashed_results[doc_hash] = (doc, similarity)

    return sorted(hashed_results.values(), key=lambda x: x[1], reverse=True)


# âœ… ê²€ìƒ‰ ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
def calculate_search_accuracy(recommended_docs, query_ingredients):
    if not recommended_docs or not query_ingredients:
        return 0.0

    query_vectors = [embedding_model.embed_query(ing) for ing in query_ingredients]
    avg_query_vector = np.mean(query_vectors, axis=0)

    similarity_scores = []
    for doc, _ in recommended_docs:
        doc_vector = embedding_model.embed_query(doc.page_content)
        similarity = cosine_similarity([doc_vector], [avg_query_vector])[0][0]
        similarity_scores.append(similarity)

    return np.mean(similarity_scores) if similarity_scores else 0.0


# âœ… ì¶”ì²œ ë‹¤ì–‘ì„± ê³„ì‚° í•¨ìˆ˜ (ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨)
def calculate_recommendation_diversity(recommended_docs):
    if len(recommended_docs) < 2:
        return 0.0  # ë¹„êµí•  ëŒ€ìƒì´ ì—†ìœ¼ë©´ 0 ë°˜í™˜

    similarities = []
    for idx, (doc1, _) in enumerate(recommended_docs):
        vec1 = embedding_model.embed_query(doc1.page_content)
        for doc2, _ in recommended_docs[idx + 1:]:
            vec2 = embedding_model.embed_query(doc2.page_content)
            sim_score = cosine_similarity([vec1], [vec2])[0][0]
            similarities.append(sim_score)

    avg_similarity = np.mean(similarities)
    std_dev = np.std(similarities)

    # í‘œì¤€í¸ì°¨ë¿ë§Œ ì•„ë‹ˆë¼ í‰ê·  ìœ ì‚¬ë„ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
    return 1 - avg_similarity + std_dev

# âœ… í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def calculate_average_similarity(recommended_docs):
    if not recommended_docs:
        return 0.0
    return np.mean([score for _, score in recommended_docs])

recommended_keywords = [
    "í† ì½”í˜ë¡¤", "ë¦¬ëª¨ë„¨", "í•˜ì´ë“œë¡œì œë„¤ì´í‹°ë“œë ˆì‹œí‹´", "íŒí…Œë†€", "ë¦¬ë‚ ë£°", "ì•„ë°ë…¸ì‹ ", "ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ", "ê¸€ë¦¬ì„¸ë¦´ì¹´í”„ë¦´ë ˆì´íŠ¸",
    "ìŠ¤í…Œì•„ë¦­ì• ì”¨ë“œ", "ì ìƒ‰ì‚°í™”ì² ", "ê¸€ë¦¬ì„¸ë¦´ìŠ¤í…Œì•„ë ˆì´íŠ¸", "ë² íƒ€ì¸", "ì•„í¬ë¦´ë ˆì´íŠ¸/C10-30ì•Œí‚¬ì•„í¬ë¦´ë ˆì´íŠ¸í¬ë¡œìŠ¤í´ë¦¬ë¨¸",
    "ì‹œíŠ¸ë¡œë„¬ì˜¬", "ì„¸ë¼ë§ˆì´ë“œì—”í”¼", "ì•Œë€í† ì¸", "ì œë¼ë‹ˆì˜¬", "ìŠ¤ì¿ ì•Œë€", "íŒ”ë¯¸í‹±ì• ì”¨ë“œ", "í‹°íƒ€ëŠ„ë””ì˜¥ì‚¬ì´ë“œ", "ë³€ì„±ì•Œì½”ì˜¬",
    "íŠ¸ë¼ì´ì—í†¡ì‹œì¹´í”„ë¦´ë¦´ì‹¤ë ˆì¸", "ë§ˆì´ì¹´", "íœí‹¸ë Œê¸€ë¼ì´ì½œ", "ë±ìŠ¤íŠ¸ë¦°", "ì‹œíŠ¸ë¦­ì• ì”¨ë“œ", "ë‹¤ì´ì•„ì´ì†ŒìŠ¤í…Œì•„ì¼ë§ë ˆì´íŠ¸",
    "ì†”ë¹„íƒ„ì•„ì´ì†ŒìŠ¤í…Œì•„ë ˆì´íŠ¸", "ì½œë ˆìŠ¤í…Œë¡¤"
]

@search_bp.route('/', methods=['GET', 'POST'])
def search():
    if request.method == 'POST':
        query = request.form.get('keywords', '').strip()
        selected_keywords = request.form.getlist('recommended_keywords')

        if not query and not selected_keywords:
            return render_template("results.html", message="ì„±ë¶„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        query_ingredients = [q.strip() for q in query.split(',') if q.strip()]
        recommended_ingredients = selected_keywords

        # ê²€ìƒ‰ ê²°ê³¼
        query_results = search_similar_cosmetics(query_ingredients, top_k=5)
        recommended_results = search_similar_cosmetics(recommended_ingredients, top_k=5)

        # ê²°ê³¼ í†µí•© ë° ì •ë ¬
        sorted_results = sorted(query_results + recommended_results, key=lambda item: item[1], reverse=True)

        # # âœ… ì •í™•ë„, ë‹¤ì–‘ì„±, í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        search_accuracy = calculate_search_accuracy(query_results, query_ingredients)
        recommendation_diversity = calculate_recommendation_diversity(sorted_results)
        average_similarity = calculate_average_similarity(sorted_results)

        # âœ… ì •í™•ë„, ë‹¤ì–‘ì„±, í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° í›„ ì„¸ì…˜ ì €ì¥
        session['search_accuracy'] = float(calculate_search_accuracy(query_results, query_ingredients))
        session['recommendation_diversity'] = float(calculate_recommendation_diversity(sorted_results))
        session['average_similarity'] = float(calculate_average_similarity(sorted_results))


        print("ğŸ” [DEBUG] ê²€ìƒ‰ ì •í™•ë„:", search_accuracy)
        print("ğŸ” [DEBUG] ì¶”ì²œ ë‹¤ì–‘ì„±:", recommendation_diversity)
        print("ğŸ” [DEBUG] í‰ê·  ìœ ì‚¬ë„:", average_similarity)

        # âœ… í„°ë¯¸ë„ì— ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
        print("\nğŸ” ê²€ìƒ‰ì–´:", query_ingredients)
        print("ğŸ“Œ ì¶”ì²œ ì„±ë¶„:", recommended_ingredients)
        print(f"ğŸ” ì´ ê²€ìƒ‰ëœ ì œí’ˆ ìˆ˜: {len(sorted_results)} ê°œ\n")
        print(f"âœ… ê²€ìƒ‰ ì •í™•ë„: {search_accuracy:.4f}")
        print(f"âœ… ì¶”ì²œ ë‹¤ì–‘ì„± (ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨): {recommendation_diversity:.4f}")
        print(f"âœ… í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜: {average_similarity:.4f}\n")


        search_accuracy = calculate_search_accuracy(query_results, query_ingredients)
        recommendation_diversity = calculate_recommendation_diversity(sorted_results)
        average_similarity = calculate_average_similarity(sorted_results)

        # âœ” ì¶œë ¥ê°’ì´ numpy.float32ë¼ë©´ float() ë³€í™˜ì´ í•„ìš”í•¨!
        # âœ” ì¶œë ¥ê°’ì´ floatì´ë©´ sessionì— ë°”ë¡œ ì €ì¥ ê°€ëŠ¥!

        print(f"ğŸ” [DEBUG] ê²€ìƒ‰ ì •í™•ë„ íƒ€ì…: {type(search_accuracy)}")
        print(f"ğŸ” [DEBUG] ì¶”ì²œ ë‹¤ì–‘ì„± íƒ€ì…: {type(recommendation_diversity)}")
        print(f"ğŸ” [DEBUG] í‰ê·  ìœ ì‚¬ë„ íƒ€ì…: {type(average_similarity)}")




        for idx, (doc, score) in enumerate(sorted_results[:10]):  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            print(f"{idx+1}. {doc.metadata['brand_name']} - {doc.metadata['product_name']}")
            print(f"   â¡ï¸ ì„¤ëª…: {doc.metadata['description']}")
            print(f"   ğŸ’° ê°€ê²©: {doc.metadata['price']}ì›")
            print(f"   ğŸ”— ë§í¬: {doc.metadata['detail_url']}")
            print(f"   ğŸ§ª ì„±ë¶„: {doc.page_content}")  # ì„±ë¶„ ì •ë³´ ì¶œë ¥
            print(f"   âœ… ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}\n")

        return render_template('results.html', query=query, results=sorted_results, selected_ingredients=query_ingredients + recommended_ingredients)

    return render_template('input.html', recommended_keywords=selected_keywords)